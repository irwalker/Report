\chapter{Policy Design and Implementation}\label{C:us}

This section outlines the design of the system, highlighting important decisions made during the course of the project, as well as the steps taken to implement the system based on the initial design. I break this section into two logical components; web-scraping implementation and policy implementation.

\section{Web Scraping Components}


\section{Language Choice}

Might take this out.

I decided fairly early in the project that Ruby was the most suitable language to implement 

I decided fairly early in the project that Ruby was the most suitable language to implement my scraper libraries with. There are a plethora of libraries to assist with web-scraping in Ruby, and its scripting background made for more straightforward scheduling and running of tasks. Alternatives considered were PHP and Java. PHP has a number of libraries to implement web-scraping tools with, however I have less experience in this language. Java would have been more efficient than Ruby, but I felt that 

Multiple approaches were considered for the implementation of my web scrapers. I decided fairly early that Ruby was the most suitable language to implement my scraper libraries with. 

\section{Architecture Design Considerations}

A standalone crawling application was conidered, in which my crawlers would be written from scratch, using libraries such as nokogiri and rest-client to request and parse pages. Alternative structures involved extending existing systems such as Ruby's scrAPI gem. I also looked at using Watir browser automation to fetch data. 

\subsection{Extend scrAPI}

scrAPI is a Rubyforge project, allowing for fast implementation of web scrapers. The benefit of scrAPI is that it would allow me to fetch data from HTML pages using CSS selectors. It also hid processes such as the actual fetching of pages, and sending of HTTP requests. Ultimately scrAPI was a more high-level approach to scraping content.

Extending scrAPI was ultimately discarded however, due to its heavy reliance on CSS files remaining constant. Any change to stylesheet files would likely have broken my scrapers. Arguably these stylesheets are less likely to change than layout manipulations (e.g. consider xpath on HTML as an alternative); however on sites such as Twitter and Facebook large design teams frequently make changes to these files. Given that a key requirement of the project was to make scrapers resistant to user-interface change, this resulted in scrAPI being deemed unfit for purpose. 

\subsection{Extend A Browser-Automation Model}

Browser automation options were considered as an alternative architecture. Libraries such as Watir allow users to 

Watir - browser automated option. More for automated tests. Performance would be significantly worse than nokogiri, as pages are loaded through the browser. Also personal experience from building such applications shows that implementation of such a solution would be very time consuming. Advantages of this would be low detection rates. 

\subsection{Standalone Crawling Application}

A standalone crawling application architecture was considered, and ultimately chosen as the most suitable strategy. 

\section{Product Implementation}

%State why I chose this architecture, and how I built the application.

%Discuss some of the difficulties I faced, Infinite Scroll, Quantifying Reputation Data, Sites Requiring Authentication.

%Sites with large amounts of interactive and dynamic content are significantly more difficult to scrape data from.

\subsection{Twitter Scraper}

%How I went about implementing the Twitter scraper

\subsection{LinkedIn Scraper}

\subsection{Facebook Scraper}





%Define the architectures that I considered for implementing web-scraper

%Option one - stand-alone web scraping libraries

%Option two - extend existing web scraper, utilise this.

\section{Policy Construction}

\subsection{MapEquation}



Two phases to each facet of implementation; scraper, data understanding, and policy implementation and evaluation. Discuss each of these in seperate sections.

Define multiple hypotheses to add strength to the report

Brief description of LinkedIn Scraper. Ultimately decided that twitter had significant data interest to be the only focus of the project.

Discuss how the project was a study on what could be inferred from the data that we can access. Therefore some dead ends, etc. E.g. LinkedIn, Sentiment140 API for tweet analysis. 

Temporal analysis

Evaluation tool development

Phases of implementation for each tool. Design, implement scraper, implement data analysis components, implement evaluation components and policies. 