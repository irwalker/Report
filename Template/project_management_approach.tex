\chapter{Methodology}\label{C:meth}

This section describes the methodology taken to complete the project. The project management and design approaches are discussed. The key project complexities are then described. 

\section{Project Management Approach}

%Define what agile approach practices were used.

%Adjust order of the initial sentences.

The project was structured around a loose Waterfall approach \cite{waterfall}. At the start of the project, a long-term plan and major milestones were outlined. More detailed plans were added and target dates adjusted as the year progressed. Aspects of Agile development practices were also used during the implementation and design stages. These included an iterative and prototyping approach to development. Prototypes were developed early in the project to investigate the feasibility of scraping various sites. From these, features were iteratively added until the final product was created.

This approach of combining aspects of both Waterfall and Agile methodologies was effective as a project management approach. Agile methods tend to work best in a team environment, assisting with the coordination of team members. However the method of small, focused sprints contributing to the larger project were excellent in maintaining focus and direction. The Waterfall structure in turn outlined the more concrete and long-term deliverables of the project, which was useful with monitoring, controlling and ensuring my work did not fall behind schedule. These target goals were met in the majority of cases.

Throughout the project I had joint weekly meetings with Dr. Kris Bubendorfer, Filip Dimitrievski, and occasionally Ferry Hendrikx. In these meetings, 30-45 minutes in length, we outlined progress achieved during the week, identified issues encountered and planned tasks for the coming week. In the latter half of the project, weekly reports were constructed in order to carefully monitor achievement and contribute to components of this report. A sample weekly report is attached as Appendix B. Having meetings with Filip and Ferry also present was beneficial, as there were aspects of the project for which Filip and I were able to collaborate over. Ferry, who developed the GRAft system was also able to give useful technical feedback, with his experience in the development of web-scraping tools. The design approach for development of these tools are discussed next.

 %CHANGE THIS TO SPECIFIC SECTION

%Design practices - strong prototyping approach. Only way to test if sites were feasible was through building prototypes. Benefit of learning how to web-scrape pages was also covered with this approach. 

%Weekly sprints, with stories to be completed by the end of each week during implementation. 

%Weekly meetings with Dr. Kris Bubendorfer and Filip Dimitrievski, as well as Ferry Hendrikx useful to get feedback. 

%Target goals - weekly goals were met the majority of the time, but focus of the project was in flux as priorities changed during implementation. 

%The project was structured around a loose waterfall approach. At the beginning of the project, a long term plan and major milestones were outlined

\section{Design Approach}

Requirements analysis and design were completed through a combination of research and prototyping. As the project was focused on an exploration of understanding reputation data on social media, a large portion of time was given to background research.

Later in the design phase a set of prototypes to evaluate the feasibility and alternatives for a web-scraping solution were constructed. These covered preliminary scrapers for Twitter, Facebook, LinkedIn and Slashdot. The benefit of developing these was to both give a better understanding of technologies involved with performing these functions, and to produce some meaningful effort early in the project, as these scrapers could be potentially useful later. 

%Talk about different design approaches that I considered, and why the approach that I took was the best.

%The weekly meetings with my supervisor allowed opportunities to obtain feedback on design choices, as well as suggestions where there was room for improvement. 

%Be more reflective here with the evaluation of my design approach. 

%Comes very close to being reflective, so would be worth just doing a reflective segment here!

%Reference? Perhaps reference a prototyping design approach.
%Prototyping to achieve understanding. Weaknesses - some wasted effort. Facebook scrapers were eventually abandoned. Twitter API changed. etc etc
%Gantt Chart Here

\section{Project Complexities}

The complexity of the system stems largely from the aggregation of unstructured data from a variety of sources - a difficulty often encountered in web-crawling applications. The codebase itself is not overly complex. Debugging unclear and unexpected errors from web servers also contributed to complexity of development.

% However, what contributed to the primary difficulty of prototype development was the development of code in the aggregation of data from disparate locations, and debugging often unclear and unexpected errors from various web requests.

Understanding the data gathered was a time-expensive challenge. The process of understanding and defining reputation data on social media was the task that occupied most of my time on the project. The selected prototyping methodology was often expensive in time, due to the necessity of revising code and collecting more relevant data.

% The time constraint of 300 hours impacted the project at all stages. The implementation and evaluation components were particularly impacted - limitations had to be placed upon the scale of data collected from scrapers in order to compensate for time. In addition, the selected prototyping methodology was often expensive in terms of time, due to the necessity of revising code and collecting more relevant data.

%Debugging and collection of data, and asserting that this data is valid. Ensuring that websites were not overloaded, resulting usually in scrapers being blocked. Recovery from detection, and how scrapers can respond. Construction of useful policies, and data analysis and aggregation. 

%Aggregation of non-structured content from a variety of sources. 

%Debugging, collection of data. Avoiding detection by websites, and ensuring that websites were not overloaded, resulting usually in scrapers being blocked.
%Recovery from detection, and how scrapers can respond. Construction of useful policies, and data analysis and aggregation.

